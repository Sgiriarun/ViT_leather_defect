{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e2a75db-6ec2-4370-9597-5b3f6b0e9c55",
   "metadata": {},
   "source": [
    "# Vision transformer models for defect detection in Leather samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea7e25-0fa9-4567-a59d-0808b8c61470",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing part\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "#TensorFlow version: 2.13.1\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import matplotlib.pyplot as plt\n",
    "import os, time\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579561ef-d606-4162-902f-af32ddd6779b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load the image data\n",
    "img_data = np.load(\"Two_class/output_image.npy\")\n",
    "#Load the Labels\n",
    "labels  = np.load('Two_class/output_labels.npy')\n",
    "\n",
    "print(\"[INFO] TRAINING SET SHAPE: {}\".format(img_data.shape))\n",
    "# TRAINING SET SHAPE: (3540, 227, 227, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa9a3d-b3aa-4f1c-98e2-dd3ee8763c16",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing data, and aggregate training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59180e8-19dc-40a3-9c1b-14fe5e507fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we need keras api (of tensorflow) for one-shot encoding, we gonna use np_utils of keras.\n",
    "num_classes = 2\n",
    "input_shape = (224, 224, 3)      \n",
    "\n",
    "# Use LabelEncoder to convert string labels to numerical labels\n",
    "label_encoder = LabelEncoder()\n",
    "numerical_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "# One-Hot Encoding of labels #\n",
    "Y = to_categorical(numerical_labels,num_classes)\n",
    "# Shuffle data              #\n",
    "x,y = shuffle(img_data,Y,random_state=2)\n",
    "# Split data - Train/Test   #\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=2)\n",
    "\n",
    "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
    "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")\n",
    "\n",
    "print(\"[INFO] PRE-PROCESSING COMPLETE --\")\n",
    "# x_train shape: (2832, 227, 227, 3) - y_train shape: (2832, 2)\n",
    "# x_test shape: (708, 227, 227, 3) - y_test shape: (708, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d94b3f-386c-417f-a1f3-cb6b2666eee7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### configuring hyper parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d01a14-e387-4f3e-b875-eb3d772a4475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate   = 0.001\n",
    "weight_decay    = 0.0001\n",
    "batch_size      = 96           # (64) reduced from 256\n",
    "num_epochs      = 100           # reduced from 100\n",
    "image_size      = 72            # We'll resize input images to this size \n",
    "patch_size      = 6             # (was 6) Size of the patches to be extract from the input images\n",
    "num_patches     = (image_size // patch_size) ** 2\n",
    "projection_dim  = 64\n",
    "num_heads       = 4             # 8?\n",
    "transformer_units = [projection_dim * 2, projection_dim, ] # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "#mlp_head_units  = [2048, 1024] # Size of the dense layers of the final classifier\n",
    "mlp_head_units  = [512, 256]    # Change mlp_head_units\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1b4dce-0941-4937-a909-8fd6e9008395",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Implement patch creation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adc303c-f8aa-4d77-8436-36e12f3814d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# include layers from layer\n",
    "class Patches(layers.Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da5ecf6-5994-4067-b9a3-5b7426e610d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement the patch encoding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58abba8d-0a75-48d8-8533-9b53e9dff624",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608ccaae-b3ff-4617-a057-c383d573415c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## self attention mechanism to get context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c410d9a7-3c9b-4efb-bf2a-7552da227f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is\n",
    "        # the square root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc2c946-ae78-4224-b13c-bc79894cfead",
   "metadata": {},
   "source": [
    "## data agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88fdeb7-0008-4b27-ba50-a6a2e5d43e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(image_size, image_size),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "## Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed5b48-8cdf-4e82-a89a-042f51d15bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement multilayer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11d41bf-5f93-4f9b-8ef0-40ef395b4d33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457ead6-b5b5-4061-9e2b-59cf3a37b590",
   "metadata": {},
   "source": [
    "## Build the diagonal attention mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196e9705-d0fc-4894-ace3-f2e683f6b4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_attn_mask = 1 - tf.eye(num_patches)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a63028c-60e0-488f-85af-6c0a2c5354b1",
   "metadata": {},
   "source": [
    "## Build the ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa864b-6891-4a98-b99b-5eafd1d0ea44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_vit_classifier(vanilla=False):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        if not vanilla:\n",
    "            print(\"[INFO] ~ MultiHeadAttentionLSA ~\")\n",
    "            attention_output = MultiHeadAttentionLSA(\n",
    "                num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "            )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        else:\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    \n",
    "    # Add MLP\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    \n",
    "    # Add final dense layer with softmax activation.\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(features)\n",
    "    \n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ef828-df48-4e87-b4ba-8ec0a1263b38",
   "metadata": {},
   "source": [
    "## Compile, train, and evaluate the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ecc80e-938d-4893-8faa-81351ae90c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_experiment(model):\n",
    "    ##----------------------------------##\n",
    "    ## ~~ MODEL EVALUATION FUNCTIONS ~~ ##\n",
    "    ##----------------------------------##\n",
    "    from keras import backend as K\n",
    "\n",
    "    def recall_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_m(y_true, y_pred):\n",
    "        precision = precision_m(y_true, y_pred)\n",
    "        recall = recall_m(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "    ##----------------------------------##\n",
    "    ##      COMPILE THE MODEL           ##\n",
    "    ##----------------------------------##\n",
    "    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])\n",
    "    model.compile(optimizer='adam', \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy', f1_m, precision_m, recall_m, tf.keras.metrics.AUC(name='auc')])\n",
    "\n",
    "\n",
    "    checkpoint_filepath = \"/checkpoint/\"    #\"/tmp/checkpoint\"\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "\n",
    "    earlystop = EarlyStopping(\n",
    "        monitor='val_accuracy', \n",
    "        patience=12, \n",
    "        mode='max', \n",
    "        verbose=1)\n",
    "\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    ##                          ~ MODEL FIT ~                       ##\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "    t=time.time()\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=batch_size,                          # 64\n",
    "        epochs=num_epochs,                              # 100\n",
    "        validation_split=0.2,                           # was 0.2\n",
    "        callbacks=[checkpoint_callback],\n",
    "    )\n",
    "    ##----------------------------------##\n",
    "    ##      EVALUATE THE MODEL          ##\n",
    "    ##----------------------------------##\n",
    "    #loss, accuracy, f1_score, precision, recall = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "    loss, accuracy, f1_score, precision, recall, auc = model.evaluate(x_test, y_test, batch_size=batch_size, verbose=0)\n",
    "\n",
    "    # View Results\n",
    "    print('[INFO] Training time: %s' % (t - time.time()))\n",
    "    print(\"[INFO] accuracy={:.4f}%\".format(accuracy * 100))\n",
    "    print(\"[INFO] loss={:.4f}\".format(loss))\n",
    "    print(\"[INFO] precision={:.4f}%\".format(precision))\n",
    "    print(\"[INFO] recall={:.4f}%\".format(recall))\n",
    "    print(\"[INFO] f1_score={:.4f}%\".format(f1_score))\n",
    "    print(\"[INFO] AUC={:.4f}%\".format(auc))\n",
    "\n",
    "    ##----------------------------------##\n",
    "    ##      RETURN METRICS & MODEL      ##\n",
    "    ##----------------------------------##\n",
    "    return history, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26958d9b-e673-4174-b6e4-716b9448d757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vit_classifier = create_vit_classifier(vanilla=False)\n",
    "history, model = run_experiment(vit_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
